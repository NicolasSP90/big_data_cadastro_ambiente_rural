{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iniciando Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Iniciar a sessão Spark com a configuração do Delta Lake\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"DeltaLake\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")  # Extensão do Delta\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")  # Catálogo do Delta\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/jars/delta-spark_2.12-3.0.0.jar,/usr/local/spark/jars/delta-storage-3.0.0.jar\")  # Caminho dos JARs no Docker\n",
    "    .config(\"spark.executor.cores\", \"2\")  # Limitar o número de núcleos por executor\n",
    "    .config(\"spark.driver.cores\", \"1\")    # Limitar o número de núcleos para o driver\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.memory.fraction\", \"0.3\")  # Ajuste para controlar a fração de memória usada pelo Spark\n",
    "    .config(\"spark.memory.storageFraction\", \"0.2\")  # Fração da memória usada para armazenar dados persistentes\n",
    "    .getOrCreate())\n",
    "\n",
    "# Verificar se a sessão Spark foi criada com sucesso\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------+--------------+--------------------+-----------------+--------------------+------------+----------------------------------+---------------------------+---------------------------+---------------------+----------------------+-----------------+-----------------+--------------------+--------------------------------+----------------------+----------------------------+-----------------+---------------+-----------------+---------------------------+----------------------------------------+----------------+-----------------------+\n",
      "| uf|           municipio|codigo_ibge|area_do_imovel|        registro_car|situacao_cadastro|   condicao_cadastro|area_liquida|area_remanescente_vegetacao_nativa|area_reserva_legal_proposta|area_preservacao_permanente|area_nao_classificada|solicitacao_adesao_pra|         latitude|        longitude|      data_inscricao|data_alteracao_condicao_cadastro|area_rural_consolidada|area_servidao_administrativa|tipo_imovel_rural|modulos_fiscais|area_uso_restrito|area_reserva_legal_averbada|area_reserva_legal_aprovada_nao_averbada|     area_pousio|data_ultima_retificacao|\n",
      "+---+--------------------+-----------+--------------+--------------------+-----------------+--------------------+------------+----------------------------------+---------------------------+---------------------------+---------------------+----------------------+-----------------+-----------------+--------------------+--------------------------------+----------------------+----------------------------+-----------------+---------------+-----------------+---------------------------+----------------------------------------+----------------+-----------------------+\n",
      "| GO|             Nazário|    5214408|      119.6326|GO-5214408-3AEF20...|               AT|Analisado com pen...|    119.6326|                  6.43202795367569|                     6.4320|           5.29751084282892|  0.00102295610380904|                   Sim|-16.5923058689987|-49.9019017039191|2014-05-07 16:01:...|                            NULL|      112.301149046683|                           0|              IRU|         5.4378|                0|                     0.0000|                                  0.0000|               0|   2014-05-07 16:01:...|\n",
      "| SC|             Meleiro|    4210803|        7.5340|SC-4210803-BC127B...|               AT|Aguardando anális...|      7.5340|                  5.51785410336982|                     0.0000|                          0|  0.00159349317984888|                   Nao|-28.7930798512303|-49.6472023744097|2014-05-07 16:02:...|                            NULL|      2.01445162492469|                           0|              IRU|         0.4186|                0|                     1.5060|                                  0.0000|               0|   2014-05-07 16:02:...|\n",
      "| GO|           Nova Roma|    5214903|       19.4883|GO-5214903-7F5804...|               PE|Analisado com pen...|     19.3600|                  19.4882633569717|                     3.8720|                          0|                    0|                   Nao|-13.6370551503248|-47.0339670619739|2014-05-07 16:06:...|                            NULL|                     0|                           0|              IRU|         0.2784|                0|                     0.0000|                                  0.0000|               0|   2014-05-07 16:06:...|\n",
      "| GO|Santa Helena de G...|    5219308|       22.9340|GO-5219308-647819...|               PE|Analisado com pen...|     22.8800|                   1.5021496193707|                     1.5028|          0.568927694143718|     18.2335997514784|                   Sim|-17.8327877579625|-50.6015145422339|2014-05-07 17:49:...|                            NULL|      3.08289042473435|                           0|              IRU|         1.1467|                0|                     0.0000|                                  0.0000|               0|   2014-05-07 17:49:...|\n",
      "| PR|   Cornélio Procópio|    4106407|       10.9560|PR-4106407-0F0608...|               AT|          Em análise|     10.9560|                                 0|                     0.0000|                          0|    0.161657419734154|                   Nao|-23.1841101335722|-50.6715645967419|2014-05-07 17:52:...|                            NULL|      10.7942669028953|                           0|              IRU|         0.6087|                0|                     0.0000|                                  0.0000|               0|   2014-05-07 17:52:...|\n",
      "| GO|     Buriti de Goiás|    5203939|       37.3628|GO-5203939-24B667...|               AT|Analisado com pen...|     37.2710|                                 0|                     7.4542|           3.48182904076294|     36.6495720619986|                   Sim|-16.2164807816082|-50.4953581696022|2014-05-07 18:10:...|                            NULL|                     0|                           0|              IRU|         1.6983|                0|                     0.0000|                                  0.0000|               0|   2014-05-07 18:10:...|\n",
      "| GO|          Planaltina|    5217609|       97.9881|GO-5217609-5AF397...|               PE|Analisado com pen...|     98.1345|                                 0|                    21.8673|           2.12858352269074|     97.6714521959102|                   Nao|-15.1179720733421|-47.8383739460241|2014-05-08 00:02:...|                            NULL|                     0|                           0|              IRU|         2.7997|                0|                     0.0000|                                  0.0000|               0|   2014-05-08 00:02:...|\n",
      "| PR|      Jandaia do Sul|    4112108|        6.1533|PR-4112108-9A8173...|               AT|Aguardando anális...|      6.1533|                                 0|                     0.0000|                          0|     6.14899874393451|                   Sim|-23.6224664264801|-51.6578666515836|2014-05-08 08:00:...|                            NULL|                     0|                           0|              IRU|         0.3846|                0|                     0.0000|                                  0.0000|               0|   2014-05-08 08:00:...|\n",
      "| PR|      Jandaia do Sul|    4112108|        4.9027|PR-4112108-A5B59A...|               AT|Aguardando anális...|      4.9027|                                 0|                     0.0000|                          0|     4.89928986130661|                   Sim|-23.6224240639671|-51.6578657246737|2014-05-08 08:03:...|                            NULL|                     0|                           0|              IRU|         0.3064|                0|                     0.0000|                                  0.0000|               0|   2014-05-08 08:03:...|\n",
      "| GO|             Formosa|    5208004|        5.4097|GO-5208004-4F872D...|               PE|Analisado com pen...|      5.4139|                                 0|                     1.1026|           2.27859310705466|     1.21704276768816|                   Sim|-15.5759058160859| -47.347320962841|2014-05-08 08:36:...|                            NULL|      3.76410267729312|                           0|              IRU|         0.1352|                0|                     0.0000|                                  0.0000|1.21609669322065|   2014-05-08 08:36:...|\n",
      "| GO|Santa Helena de G...|    5219308|        3.9105|GO-5219308-AF2363...|               PE|Analisado com pen...|      3.9017|                 0.245592305220291|                     0.2439|                          0|     3.11964077305441|                   Sim|  -17.76391298631|-50.7645585662756|2014-05-08 08:41:...|                            NULL|      0.53817772397697|                           0|              IRU|         0.1955|                0|                     0.0000|                                  0.0000|               0|   2014-05-08 08:41:...|\n",
      "| PR|             Astorga|    4102109|      213.8016|PR-4102109-2E8FF9...|               AT|Aguardando anális...|    213.8016|                                 0|                     0.0000|           15.1908326805634|     128.481619369247|                   Nao|-23.3388741895933|-51.6458730432584|2014-05-08 09:03:...|                            NULL|      83.6068667104104|                           0|              IRU|        13.3626|                0|                   103.3418|                                  0.0000|               0|   2014-05-08 09:03:...|\n",
      "| GO|             Itapaci|    5210901|      177.3984|GO-5210901-18295A...|               AT|Analisado com pen...|    177.3984|                                 0|                    32.4073|           9.65986396424072|     175.233119654483|                   Nao|-15.0467079484747|-49.8101753503116|2014-05-08 10:37:...|                            NULL|                     0|                           0|              IRU|         5.9133|                0|                    29.4853|                                  0.0000|               0|   2014-05-08 10:37:...|\n",
      "| GO|           Morrinhos|    5213806|        6.8385|GO-5213806-B36B17...|               AT|Analisado com pen...|      6.8385|                  6.49708147328459|                     1.3888|          0.770908967393169|    0.238097099980264|                   Nao|-17.9207537479263|-48.9040563161888|2014-05-08 10:40:...|                            NULL|                     0|                           0|              IRU|         0.1710|                0|                     0.0000|                                  0.0000|               0|   2014-05-08 10:40:...|\n",
      "| GO|           Goianésia|    5208608|       20.3658|GO-5208608-9D8E6D...|               PE|Analisado com pen...|     20.3016|                                 0|                     4.1670|           2.89464033310083|      20.036599391474|                   Nao|-15.5639038390729|-49.1886739921727|2014-05-08 10:53:...|                            NULL|                     0|                           0|              IRU|         1.0183|                0|                     0.0000|                                  0.0000|               0|   2014-05-08 10:53:...|\n",
      "| RJ|           Itaperuna|    3302205|        6.0851|RJ-3302205-169354...|               PE|Analisado com pen...|      5.9310|                                 0|                     1.4792|           1.61754906789992|     5.83886665807348|                   Sim|-21.3214432276222|  -41.80444759989|2014-05-08 11:02:...|                            NULL|    0.0501235222115647|                      0.1619|              IRU|         0.2028|                0|                     0.0000|                                  0.0000|4.33622302031857|   2014-05-08 11:02:...|\n",
      "| GO|           Goianésia|    5208608|       20.3658|GO-5208608-A154F2...|               PE|Analisado com pen...|     20.3016|                                 0|                     4.1670|           2.89464033310083|      20.036599391474|                   Nao|-15.5639038390729|-49.1886739921727|2014-05-08 11:07:...|                            NULL|                     0|                           0|              IRU|         1.0183|                0|                     0.0000|                                  0.0000|               0|   2014-05-08 11:07:...|\n",
      "| PR|Marechal Cândido ...|    4114609|        3.3549|PR-4114609-2D4A70...|               AT|Aguardando anális...|      3.3549|                                 0|                     0.0000|                          0|     3.35944271401332|                   Sim|-24.5530118986487|-54.0842691356302|2014-05-08 11:14:...|                            NULL|                     0|                           0|              IRU|         0.1864|                0|                     0.0000|                                  0.0000|               0|   2014-05-08 11:14:...|\n",
      "| MA|            Pinheiro|    2108603|      291.6915|MA-2108603-3C550F...|               PE|Analisado com pen...|    287.8908|                   234.09707247566|                   230.4369|                          0|     5.47498268246462|                   Nao|-2.30715012142595| -45.002070084726|2014-05-08 11:51:...|            2021-12-17 12:28:...|      52.0483564388429|                           0|              IRU|         5.3035|                0|                     0.0000|                                  0.0000|               0|   2014-05-08 11:51:...|\n",
      "| RJ|           Itaperuna|    3302205|       44.4215|RJ-3302205-1DEA02...|               PE|Analisado com pen...|     43.3195|                                 0|                     8.8523|           11.0546352649015|     40.9607595756084|                   Sim|-21.3400673460579|-41.6868689432052|2014-05-08 12:03:...|                            NULL|     0.113328063728078|                      1.1514|              IRU|         1.4807|                0|                     0.0000|                                  0.0000|31.9864053125698|   2014-05-08 12:03:...|\n",
      "+---+--------------------+-----------+--------------+--------------------+-----------------+--------------------+------------+----------------------------------+---------------------------+---------------------------+---------------------+----------------------+-----------------+-----------------+--------------------+--------------------------------+----------------------+----------------------------+-----------------+---------------+-----------------+---------------------------+----------------------------------------+----------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.read.csv(\"/home/jovyan/app/data/raw/temas_ambientais.csv\", sep=\";\", header=True)\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvando Bronze (Distribuída)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.write.mode(\"overwrite\").format(\"delta\").save(\"/home/jovyan/app/data/bronze/bronze_distribuida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpeza e Salvando Silver (Distribuída)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-----------+--------------+--------------------+-----------------+--------------------+------------+----------------------------------+---------------------------+---------------------------+---------------------+----------------------+-----------------+-----------------+--------------------+--------------------------------+----------------------+----------------------------+-----------------+---------------+-----------------+---------------------------+----------------------------------------+-----------+-----------------------+\n",
      "| uf|        municipio|codigo_ibge|area_do_imovel|        registro_car|situacao_cadastro|   condicao_cadastro|area_liquida|area_remanescente_vegetacao_nativa|area_reserva_legal_proposta|area_preservacao_permanente|area_nao_classificada|solicitacao_adesao_pra|         latitude|        longitude|      data_inscricao|data_alteracao_condicao_cadastro|area_rural_consolidada|area_servidao_administrativa|tipo_imovel_rural|modulos_fiscais|area_uso_restrito|area_reserva_legal_averbada|area_reserva_legal_aprovada_nao_averbada|area_pousio|data_ultima_retificacao|\n",
      "+---+-----------------+-----------+--------------+--------------------+-----------------+--------------------+------------+----------------------------------+---------------------------+---------------------------+---------------------+----------------------+-----------------+-----------------+--------------------+--------------------------------+----------------------+----------------------------+-----------------+---------------+-----------------+---------------------------+----------------------------------------+-----------+-----------------------+\n",
      "| AM|           Lábrea|    1302405|       82.9099|AM-1302405-60C132...|               PE|Analisado com pen...|     82.9098|                  49.0594596577163|                    66.2808|           2.92893049846099|     17.2212823151367|                   Sim|-8.82441366302527|-66.7203153604377|2014-08-18 13:17:...|            2022-08-11 09:19:...|      16.1401565513901|                           0|              AST|         0.8291|                0|                     0.0000|                                  0.0000|          0|   2014-08-18 13:17:...|\n",
      "| MA|        São Bento|    2110500|     3914.6239|MA-2110500-D11369...|               PE|Analisado com pen...|   3911.3655|                                 0|                     0.0000|                          0|     3911.38835785304|                   Nao|-2.83546096820604|-45.0704518287366|2014-08-29 10:30:...|            2021-08-20 10:36:...|                     0|                           0|              IRU|        71.1750|                0|                     0.0000|                                  0.0000|          0|   2014-08-29 10:30:...|\n",
      "| AC|            Feijó|    1200302|       44.0566|AC-1200302-E1DDA4...|               PE|Analisado com pen...|     44.0566|                  27.6701256134942|                     0.0000|                          0|   0.0344355503874977|                   Nao|-8.21028562491248|-70.2350991000402|2014-10-20 22:07:...|            2022-01-12 11:47:...|      16.3519375305264|                           0|              IRU|         0.4406|                0|                     0.0000|                                  0.0000|          0|   2014-10-20 22:07:...|\n",
      "| MT|    Alta Floresta|    5100250|       39.3635|MT-5100250-080A49...|               PE|Analisado com pen...|     39.3635|                  3.85235561523438|                     3.8524|           8.43652768554688|      35.246773762831|                   Nao|-9.68213805175886|-56.2314455726238|2014-10-22 02:29:...|            2021-11-30 15:22:...|                     0|                           0|              IRU|         0.3936|                0|                     0.0000|                                  0.0000|          0|   2014-10-22 02:29:...|\n",
      "| MT|          Tapurah|    5108006|      510.9969|MT-5108006-863BFD...|               AT|Analisado com pen...|    510.9969|                  249.907166210938|                   249.9072|           7.83154116210938|     261.092095898566|                   Nao|-12.2719274016526|-56.3539554111014|2014-10-22 04:41:...|            2021-11-28 03:37:...|                     0|                           0|              IRU|         5.1100|                0|                     0.0000|                                  0.0000|          0|   2014-10-22 04:41:...|\n",
      "| SP|        Arco-Íris|    3503356|       60.5745|SP-3503356-F0489E...|               AT|Revisado, aguarda...|     60.5310|                                 0|                     0.0000|                          0|     60.5158120007595|                   Nao|-21.7416707440427|-50.4486015536734|2014-11-15 22:59:...|            2022-04-08 17:01:...|                     0|                           0|              IRU|         3.0287|                0|                     0.0000|                                  0.0000|          0|   2014-11-17 22:57:...|\n",
      "| SP|            Matão|    3529302|        2.1734|SP-3529302-6BFA76...|               AT|Revisado, aguarda...|      2.1756|                                 0|                     0.0000|                          0|     2.17559332323338|                   Nao|-21.5835194131746|-48.3936632884126|2014-11-15 23:04:...|            2022-02-04 09:23:...|                     0|                           0|              IRU|         0.1811|                0|                     0.0000|                                  0.0000|          0|   2014-11-17 23:02:...|\n",
      "| SP|         Guapiara|    3517604|      398.9194|SP-3517604-886E15...|               AT|Revisado, aguarda...|    391.9408|                  116.684347490692|                     0.0000|                          0|      270.75866189252|                   Nao|-24.1505309663166|-48.4981964065886|2014-11-15 23:11:...|            2022-09-09 05:13:...|        4.148183984375|            7.29066806640625|              IRU|        24.4768|                0|                     0.0000|                                  0.0000|          0|   2014-11-17 23:10:...|\n",
      "| SP|          Guaraci|    3517901|       45.4524|SP-3517901-FA714D...|               AT|Revisado, aguarda...|     44.1543|                                 0|                     0.0000|                          0|     44.1543033300598|                   Nao|-20.5411065920448|-48.9533778298427|2014-11-15 23:46:...|            2022-09-09 05:46:...|                     0|                1.3124328125|              IRU|         1.4713|                0|                     0.0000|                                  0.0000|          0|   2014-11-18 22:24:...|\n",
      "| SP|Alfredo Marcondes|    3500808|       48.2747|SP-3500808-C4EDB3...|               AT|Revisado, aguarda...|     47.8503|                  1.05830142822266|                     0.0000|                          0|     45.5003467240862|                   Nao|-21.9327166234325|-51.3861386998586|2014-11-16 22:55:...|            2022-09-10 17:18:...|      1.29041671624184|           0.387990161132812|              IRU|         2.1767|                0|                     0.0000|                                  0.0000|          0|   2014-11-19 23:08:...|\n",
      "+---+-----------------+-----------+--------------+--------------------+-----------------+--------------------+------------+----------------------------------+---------------------------+---------------------------+---------------------+----------------------+-----------------+-----------------+--------------------+--------------------------------+----------------------+----------------------------+-----------------+---------------+-----------------+---------------------------+----------------------------------------+-----------+-----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.read.format(\"delta\").load(\"/home/jovyan/app/data/bronze/bronze_distribuida\")\n",
    "df_test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uf: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- codigo_ibge: string (nullable = true)\n",
      " |-- area_do_imovel: string (nullable = true)\n",
      " |-- registro_car: string (nullable = true)\n",
      " |-- situacao_cadastro: string (nullable = true)\n",
      " |-- condicao_cadastro: string (nullable = true)\n",
      " |-- area_liquida: string (nullable = true)\n",
      " |-- area_remanescente_vegetacao_nativa: string (nullable = true)\n",
      " |-- area_reserva_legal_proposta: string (nullable = true)\n",
      " |-- area_preservacao_permanente: string (nullable = true)\n",
      " |-- area_nao_classificada: string (nullable = true)\n",
      " |-- solicitacao_adesao_pra: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- data_inscricao: string (nullable = true)\n",
      " |-- data_alteracao_condicao_cadastro: string (nullable = true)\n",
      " |-- area_rural_consolidada: string (nullable = true)\n",
      " |-- area_servidao_administrativa: string (nullable = true)\n",
      " |-- tipo_imovel_rural: string (nullable = true)\n",
      " |-- modulos_fiscais: string (nullable = true)\n",
      " |-- area_uso_restrito: string (nullable = true)\n",
      " |-- area_reserva_legal_averbada: string (nullable = true)\n",
      " |-- area_reserva_legal_aprovada_nao_averbada: string (nullable = true)\n",
      " |-- area_pousio: string (nullable = true)\n",
      " |-- data_ultima_retificacao: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uf',\n",
       " 'municipio',\n",
       " 'codigo_ibge',\n",
       " 'area_do_imovel',\n",
       " 'registro_car',\n",
       " 'situacao_cadastro',\n",
       " 'condicao_cadastro',\n",
       " 'area_liquida',\n",
       " 'area_remanescente_vegetacao_nativa',\n",
       " 'area_reserva_legal_proposta',\n",
       " 'area_preservacao_permanente',\n",
       " 'area_nao_classificada',\n",
       " 'solicitacao_adesao_pra',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'data_inscricao',\n",
       " 'data_alteracao_condicao_cadastro',\n",
       " 'area_rural_consolidada',\n",
       " 'area_servidao_administrativa',\n",
       " 'tipo_imovel_rural',\n",
       " 'modulos_fiscais',\n",
       " 'area_uso_restrito',\n",
       " 'area_reserva_legal_averbada',\n",
       " 'area_reserva_legal_aprovada_nao_averbada',\n",
       " 'area_pousio',\n",
       " 'data_ultima_retificacao']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_columns = [\"codigo_ibge\"]\n",
    "float_columns = [\"area_do_imovel\", \"area_liquida\", \"area_remanescente_vegetacao_nativa\", \n",
    "                 \"area_reserva_legal_proposta\", \"area_preservacao_permanente\", \"area_nao_classificada\",\n",
    "                 \"latitude\", \"longitude\", \"area_rural_consolidada\", \"area_servidao_administrativa\",\n",
    "                 \"modulos_fiscais\", \"area_uso_restrito\", \"area_reserva_legal_averbada\",\n",
    "                 \"area_reserva_legal_aprovada_nao_averbada\", \"area_pousio\"]\n",
    "bool_columns = [\"solicitacao_adesao_pra\"]\n",
    "date_columns = [\"data_inscricao\", \"data_alteracao_condicao_cadastro\", \"data_ultima_retificacao\"]\n",
    "text_column = [x for x in df_test.columns if x not in int_columns+float_columns+bool_columns+date_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- uf: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- codigo_ibge: integer (nullable = true)\n",
      " |-- area_do_imovel: float (nullable = true)\n",
      " |-- registro_car: string (nullable = true)\n",
      " |-- situacao_cadastro: string (nullable = true)\n",
      " |-- condicao_cadastro: string (nullable = true)\n",
      " |-- area_liquida: float (nullable = true)\n",
      " |-- area_remanescente_vegetacao_nativa: float (nullable = true)\n",
      " |-- area_reserva_legal_proposta: float (nullable = true)\n",
      " |-- area_preservacao_permanente: float (nullable = true)\n",
      " |-- area_nao_classificada: float (nullable = true)\n",
      " |-- solicitacao_adesao_pra: boolean (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- data_inscricao: date (nullable = true)\n",
      " |-- data_alteracao_condicao_cadastro: date (nullable = true)\n",
      " |-- area_rural_consolidada: float (nullable = true)\n",
      " |-- area_servidao_administrativa: float (nullable = true)\n",
      " |-- tipo_imovel_rural: string (nullable = true)\n",
      " |-- modulos_fiscais: float (nullable = true)\n",
      " |-- area_uso_restrito: float (nullable = true)\n",
      " |-- area_reserva_legal_averbada: float (nullable = true)\n",
      " |-- area_reserva_legal_aprovada_nao_averbada: float (nullable = true)\n",
      " |-- area_pousio: float (nullable = true)\n",
      " |-- data_ultima_retificacao: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NULL para None\n",
    "for i in df_test.columns:\n",
    "    df_test = df_test.withColumn(i, \n",
    "                                 when(col(i) == \"NULL\", None).\n",
    "                                 otherwise(col(i)))\n",
    "\n",
    "\n",
    "# Dados Para Integer\n",
    "df_test = df_test.withColumn(\"codigo_ibge\", col(\"codigo_ibge\").cast(IntegerType()))\n",
    "\n",
    "# Dados Para Float\n",
    "for i in float_columns:\n",
    "    df_test = df_test.withColumn(i, col(i).cast(FloatType()))\n",
    "\n",
    "# Dados para Bool\n",
    "df_test = df_test.withColumn(\"solicitacao_adesao_pra\", \n",
    "                             when(col(\"solicitacao_adesao_pra\") == \"Sim\", True).\n",
    "                             when(col(\"solicitacao_adesao_pra\") == \"Não\", False).\n",
    "                             otherwise(None))\n",
    "\n",
    "# Dados para Date\n",
    "for i in date_columns:\n",
    "    df_test = df_test.withColumn(i, to_date(i, \"yyyy-MM-dd\"))\n",
    "\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2040.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 101.0 failed 1 times, most recent failure: Lost task 0.0 in stage 101.0 (TID 1323) (bb61fc89c10a executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2014-06-03 21:07:56.251' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.time.format.DateTimeParseException: Text '2014-06-03 21:07:56.251' could not be parsed, unparsed text found at index 10\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.GeneratedMethodAccessor149.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2014-06-03 21:07:56.251' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text '2014-06-03 21:07:56.251' could not be parsed, unparsed text found at index 10\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)\n\t... 21 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2040.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 101.0 failed 1 times, most recent failure: Lost task 0.0 in stage 101.0 (TID 1323) (bb61fc89c10a executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2014-06-03 21:07:56.251' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.time.format.DateTimeParseException: Text '2014-06-03 21:07:56.251' could not be parsed, unparsed text found at index 10\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.GeneratedMethodAccessor149.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '2014-06-03 21:07:56.251' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:194)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text '2014-06-03 21:07:56.251' could not be parsed, unparsed text found at index 10\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1952)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:192)\n\t... 21 more\n"
     ]
    }
   ],
   "source": [
    "df_test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
